{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9545/105393662.py:2: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv('physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv.gz', nrows=10000, compression='gzip',usecols=['TEXT'],\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('physionet.org/files/mimiciii/1.4/NOTEEVENTS.csv.gz', nrows=10000, compression='gzip',usecols=['TEXT'],\n",
    "                   error_bad_lines=False)\n",
    "# Just get 1000 data points and only the text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_word = ['very','pleasant'] #\n",
    "pattern = '|'.join(key_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['check'] = data['TEXT'].str.contains(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     7212\n",
       "False    2788\n",
       "Name: check, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.check.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Admission Date:  [**2151-7-16**]       Dischar...\n",
       "1    Admission Date:  [**2118-6-2**]       Discharg...\n",
       "2    Admission Date:  [**2119-5-4**]              D...\n",
       "3    Admission Date:  [**2124-7-21**]              ...\n",
       "4    Admission Date:  [**2162-3-3**]              D...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.TEXT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9545/2491769875.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['TEXT'] = data.TEXT.str.replace('[^a-zA-Z1-9]', ' ') # Basic cleaning to remove *or -, can be further customized later\n"
     ]
    }
   ],
   "source": [
    "data['TEXT'] = data.TEXT.str.replace('[^a-zA-Z1-9]', ' ') # Basic cleaning to remove *or -, can be further customized later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Admission Date      2151 7 16          Dischar...\n",
       "1    Admission Date      2118 6 2          Discharg...\n",
       "2    Admission Date      2119 5 4                 D...\n",
       "3    Admission Date      2124 7 21                 ...\n",
       "4    Admission Date      2162 3 3                 D...\n",
       "Name: TEXT, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['TEXT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Doc_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admission', 'date', '2151', '7', '16', 'discharge', 'date', '2151', '8', '4', 'service', 'addendum', 'radiologic', 'studies', 'radiologic', 'studies', 'also', 'included', 'a', 'chest', 'ct', 'which', 'confirmed', 'cavitary', 'lesions', 'in', 'the', 'left', 'lung', 'apex', 'consistent', 'with', 'infectious', 'process', 'tuberculosis', 'this', 'also', 'moderate', 'sized', 'left', 'pleural', 'effusion', 'head', 'ct', 'head', 'ct', 'showed', 'no', 'intracranial', 'hemorrhage', 'or', 'mass', 'effect', 'but', 'old', 'infarction', 'consistent', 'with', 'past', 'medical', 'history', 'abdominal', 'ct', 'abdominal', 'ct', 'showed', 'lesions', 'of', 't1', 'and', 'sacrum', 'most', 'likely', 'secondary', 'to', 'osteoporosis', 'these', 'can', 'be', 'followed', 'by', 'repeat', 'imaging', 'as', 'an', 'outpatient', 'first', 'name8', 'namepattern2', 'first', 'name4', 'namepattern1', '1775', 'last', 'name', 'namepattern1', 'm', 'd', 'md', 'number', '1', '1776', 'dictated', 'by', 'hospital', '18', '7', 'medquist36', 'd', '2151', '8', '5', '12', '11', 't', '2151', '8', '5', '12', '21', 'job', 'job', 'number', '18', '8']\n"
     ]
    }
   ],
   "source": [
    "sentences = data.TEXT.values\n",
    "tokenized_sent = []\n",
    "\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "print(tokenized_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvector_size = Dimensionality of the feature vectors.\\nwindow = The maximum distance between the current and predicted word within a sentence.\\nmin_count = Ignores all words with total frequency lower than this.\\nalpha = The initial learning rate.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d2v = Doc2Vec(tagged_data, vector_size = 200, window = 2, min_count = 10, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emb'] = data['tokens'].apply(model_d2v.infer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>check</th>\n",
       "      <th>tokens</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admission Date      2151 7 16          Dischar...</td>\n",
       "      <td>False</td>\n",
       "      <td>[admission, date, 2151, 7, 16, discharge, date...</td>\n",
       "      <td>[0.9194876, 0.31617367, 1.6602403, -0.8010895,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Date      2118 6 2          Discharg...</td>\n",
       "      <td>False</td>\n",
       "      <td>[admission, date, 2118, 6, 2, discharge, date,...</td>\n",
       "      <td>[-0.28176105, -1.3940994, 2.023058, 1.3280034,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Admission Date      2119 5 4                 D...</td>\n",
       "      <td>True</td>\n",
       "      <td>[admission, date, 2119, 5, 4, discharge, date,...</td>\n",
       "      <td>[0.34921858, -1.352876, 1.4489717, 2.0817325, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Admission Date      2124 7 21                 ...</td>\n",
       "      <td>True</td>\n",
       "      <td>[admission, date, 2124, 7, 21, discharge, date...</td>\n",
       "      <td>[1.0518075, -1.3964953, 0.12544218, -0.3224223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admission Date      2162 3 3                 D...</td>\n",
       "      <td>True</td>\n",
       "      <td>[admission, date, 2162, 3, 3, discharge, date,...</td>\n",
       "      <td>[-0.7803605, -0.006952791, 1.2848545, -0.52551...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  check  \\\n",
       "0  Admission Date      2151 7 16          Dischar...  False   \n",
       "1  Admission Date      2118 6 2          Discharg...  False   \n",
       "2  Admission Date      2119 5 4                 D...   True   \n",
       "3  Admission Date      2124 7 21                 ...   True   \n",
       "4  Admission Date      2162 3 3                 D...   True   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [admission, date, 2151, 7, 16, discharge, date...   \n",
       "1  [admission, date, 2118, 6, 2, discharge, date,...   \n",
       "2  [admission, date, 2119, 5, 4, discharge, date,...   \n",
       "3  [admission, date, 2124, 7, 21, discharge, date...   \n",
       "4  [admission, date, 2162, 3, 3, discharge, date,...   \n",
       "\n",
       "                                                 emb  \n",
       "0  [0.9194876, 0.31617367, 1.6602403, -0.8010895,...  \n",
       "1  [-0.28176105, -1.3940994, 2.023058, 1.3280034,...  \n",
       "2  [0.34921858, -1.352876, 1.4489717, 2.0817325, ...  \n",
       "3  [1.0518075, -1.3964953, 0.12544218, -0.3224223...  \n",
       "4  [-0.7803605, -0.006952791, 1.2848545, -0.52551...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('TimKond/S-BioLinkBert-MedQuAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.277509    0.26719996 -0.0105282  ... -0.12347386  1.0648662\n",
      "  -0.17493397]\n",
      " [ 0.03479018  0.18932167 -0.02936742 ... -0.05296332  0.73083\n",
      "  -0.21440667]\n",
      " [-0.02235499  0.33601063  0.13468382 ... -0.03416088  0.07084354\n",
      "  -0.35041183]\n",
      " ...\n",
      " [-0.1237298   0.33057767  0.28832722 ... -0.15255748  0.23030174\n",
      "  -0.4573601 ]\n",
      " [ 0.04183475  0.24472262  0.10942968 ... -0.05232212  0.44331408\n",
      "  -0.23331973]\n",
      " [-0.05767366  0.21234523  0.21282771 ...  0.09578045 -0.10990594\n",
      "  -0.38771906]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "def clustering_question(data,emb_col,NUM_CLUSTERS = 2):\n",
    "\n",
    "    X = np.array(data[emb_col].tolist())\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        repeats=25,avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    data['cluster'] = pd.Series(assigned_clusters, index=data.index)\n",
    "    data['centroid'] = data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d2v = clustering_question(data,'emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster  check\n",
       "0        False     88\n",
       "         True     610\n",
       "1        False    170\n",
       "         True     132\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_d2v.groupby([\"cluster\", \"check\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emb_transformer'] = data['TEXT'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_trans = clustering_question(data,'emb_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster  check\n",
       "0        False    189\n",
       "         True     550\n",
       "1        False     69\n",
       "         True     192\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_trans.groupby([\"cluster\", \"check\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Neural Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_string(s):\n",
    "    # Remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    # Replace all runs of whitespaces with no space\n",
    "    s = re.sub(r\"\\s+\", '', s)\n",
    "    # replace digits with no space\n",
    "    s = re.sub(r\"\\d\", '', s)\n",
    "\n",
    "    return s\n",
    "\n",
    "def tockenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            word = preprocess_string(word)\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:5000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
    "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label == True else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label == False else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (7500,)\n",
      "shape of test data is (2500,)\n"
     ]
    }
   ],
   "source": [
    "X,y = data['TEXT'].values,data['check'].values\n",
    "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gwan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9545/696823680.py:53: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 5000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    7500.000000\n",
       "mean      946.539733\n",
       "std       492.109023\n",
       "min         6.000000\n",
       "25%       593.000000\n",
       "50%       879.500000\n",
       "75%      1221.000000\n",
       "max      4393.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = padding_(x_train,3000)\n",
    "x_test_pad = padding_(x_test,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
    "        super(SentimentRNN,self).__init__()\n",
    " \n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    " \n",
    "        self.no_layers = no_layers\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #lstm\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
    "                           num_layers=no_layers, batch_first=True)\n",
    "        \n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size = x.size(0)\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
    "        #print(embeds.shape)  #[50, 500, 1000]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
    "        hidden = (h0,c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(5001, 64)\n",
      "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "no_layers = 2\n",
    "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
    "embedding_dim = 64\n",
    "output_dim = 1\n",
    "hidden_dim = 256\n",
    "\n",
    "\n",
    "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
    "\n",
    "#moving to gpu\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# function to predict accuracy\n",
    "def acc(pred,label):\n",
    "    pred = torch.round(pred.squeeze())\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 5 \n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "    # initialize hidden state \n",
    "    h = model.init_hidden(batch_size)\n",
    "    for inputs, labels in train_loader:\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)   \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        output,h = model(inputs,h)\n",
    "        \n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        # calculating accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    " \n",
    "    \n",
    "        \n",
    "    val_h = model.init_hidden(batch_size)\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "    model.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "            val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "            val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "            \n",
    "            accuracy = acc(output,labels)\n",
    "            val_acc += accuracy\n",
    "            \n",
    "    epoch_train_loss = np.mean(train_losses)\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        torch.save(model.state_dict(), '../working/state_dict.pt')\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(25*'==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n",
    "                         if preprocess_string(word) in vocab.keys()])\n",
    "        word_seq = np.expand_dims(word_seq,axis=0)\n",
    "        pad =  torch.from_numpy(padding_(word_seq,500))\n",
    "        inputs = pad.to(device)\n",
    "        batch_size = 1\n",
    "        h = model.init_hidden(batch_size)\n",
    "        h = tuple([each.data for each in h])\n",
    "        output, h = model(inputs, h)\n",
    "        return(output.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e785c52e1a28ad980bef155e7f9c26fcc6dca216df0c4e690729e404ede1f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
