{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "# Just get 1000 data points and only the text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.7'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key_word = ['very','pleasant'] #\n",
    "# pattern = '|'.join(key_word)\n",
    "# data['check'] = data['TEXT'].str.contains(pattern)\n",
    "# data.check.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Doc_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['full', 'code', 'nkda', '55', 'y', 'o', 'f', 'with', 'morbid', 'obesity', 'and', 'htn', 'no', 'recent', 'medical', 'care', 'admitted', 'to', 'hospital1', 'ed', 'on', '1', '2', 'w', 'ftt', 'and', 'somnolence', 'found', 'to', 'have', 'large', 'stage', 'iv', 'pressure', 'ulcers', 'on', 'rlq', 'and', 'ru', 'thigh', 'wounds', 'first', 'debrided', 'by', 'plastic', 'doctor', 'first', 'name', 'in', 'ed', 'admitted', 'to', 'hospital', 'unit', 'name', '4', 'with', 'transient', 'hypotension', 'lethargy', 'and', 'nursing', 'wound', 'care', 'issues', 'decubitus', 'ulcer', 'present', 'at', 'admission', 'assessment', 'continues', 'with', 'increased', 'eschar', 'on', 'rlq', 'malodorous', 'other', 'abdominal', 'wounds', 'improved', 'occasionally', 'bleeding', 'seen', 'by', 'plastics', 'dr', 'last', 'name', 'stitle', '11562', 'this', 'am', 'beeper', 'numeric', 'identifier', '11563', 'who', 'recommended', 'tid', 'dressing', 'changes', 'to', 'rlq', 'area', 'xeroform', 'gauze', 'placed', 'by', 'plastics', 'on', 'abdominal', 'area', 'with', 'excoriation', 'small', 'amt', 'bleeding', 'which', 'they', 'recommend', 'be', 'changed', 'daily', 'with', 'softsorb', 'on', 'top', 'of', 'area', 'action', 'pt', 'premedicated', 'with', '4mg', 'iv', 'morphine', '15', 'minutes', 'prior', 'to', 'dressing', 'changes', 'rlq', 'wounds', 'cleansed', 'with', 'skin', 'integrity', 'wound', 'cleanser', 'dressings', 'changed', 'with', 'strength', 'dakins', 'kerlex', 'abd', 'pads', 'and', 'softsorb', 'x1', 'this', 'shift', 'criticaid', 'antifungal', 'applied', 'to', 'remainder', 'of', 'pannus', 'r', 'buttock', 'thighs', 'and', 'creases', 'lower', 'extremities', 'cleansed', 'with', 'foam', 'and', 'covered', 'with', 'aloe', 'vesta', 'b', 'l', 'administered', 'iv', 'abx', 'as', 'ordered', 'am', 'vanco', 'level', '16', '4', 'this', 'am', 'iv', 'vanco', 'given', 'at', '12pm', 'after', 'level', 'back', 'and', 'confirmed', 'with', 'dr', 'last', 'name', 'stitle', '11565', 'response', 'ongoing', 'no', 'significant', 'change', 'plan', 'plastics', 'service', 'to', 'continue', 'to', 'follow', 'pt', 'they', 'would', 'like', 'to', 'continue', 'the', 'dressing', 'changes', 'to', 'rlq', 'tid', '6am', '2pm', '10pm', 'and', 'wait', 'a', 'few', 'days', 'to', 'clean', 'up', 'the', 'wound', 'before', 'they', 'would', 'debride', 'continue', 'to', 'premedicate', 'with', 'iv', 'morphine', '15minutes', 'prior', 'to', 'dressing', 'changes', 'continue', 'abx', 'and', 'follow', 'vanco', 'level', 'dose', 'accordingly', 'depression', 'assessment', 'labile', 'affect', 'expressing', 'frustration', 'apprehensive', 'around', 'wound', 'care', 'sometimes', 'stating', 'initials', 'namepattern4', 'last', 'name', 'namepattern4', 't', 'know', 'what', 'i', 'want', 'but', 'ultimately', 'cooperative', 'with', 'most', 'of', 'nursing', 'care', 'sw', 'and', 'psych', 'following', 'pt', 'pt', 'continues', 'on', 'celexa', 'action', 'emotional', 'support', 'often', 'provided', 'continues', 'on', 'celexa', 'response', 'minimal', 'change', 'continues', 'to', 'exhibit', 'both', 'frustration', 'and', 'gratitiude', 'for', 'care', 'plan', 'continue', 'celexa', 'may', 'need', 'to', 'increase', 'dose', 'psych', 'and', 'sw', 'continue', 'to', 'follow', 'continue', 'emotional', 'support', 'encouragement', 'hypovolemia', 'volume', 'depletion', 'without', 'shock', 'assessment', 'pt', 'appearing', 'slightly', 'dry', 'c', 'o', 'being', 'thirsty', 'asking', 'for', 'ice', 'chips', 'and', 'drinks', 'having', 'short', 'runs', 'svt', 'up', 'to', '120s', 'but', 'quickly', 'back', 'down', 'to', '90s', 'sr', 'rbbb', 'occasional', 'pvcs', 'and', 'apcs', 'maintaining', 'adequate', 'u', 'o', '30cc', 'hr', 'action', 'po', 'fluids', 'pushed', 'pt', 'taking', 'adequate', 'amt', 'po', 'fluid', 'intake', 'with', 'encouragement', 'metoprolol', 'standing', 'dose', 'given', 'early', 'this', 'am', 'for', 'short', 'runs', 'svt', 'per', 'dr', 'last', 'name', 'stitle', '11565', 'given', 'hear', 'healthy', 'diet', 'with', 'glucose', 'control', 'boost', 'response', 'stable', 'bp', 'with', 'good', 'u', 'o', 'plan', 'continue', 'to', 'encourage', 'po', 'intake', 'glucose', 'control', 'boost', 'with', 'heart', 'healthy', 'diet', 'neuro', 'a', 'ox3', 'has', 'had', 'some', 'documentation', 'of', 'confusion', 'but', 'none', 'noted', 'this', 'shift', 'pt', 'appropriate', 'following', 'commands', 'very', 'anxious', 'and', 'overwhelmed', 'at', 'times', 'with', 'any', 'new', 'change', 'needs', 'a', 'lot', 'of', 'reassurance', 'requires', 'assist', 'with', 'adls', 'able', 'to', 'feed', 'self', 'and', 'help', 'with', 'turns', 'with', 'lift', 'device', 'with', '2', 'assist', 'to', 'use', 'lift', 'cv', 'hr', 'ranging', '90s', 'occasionally', '100s', 'sr', 'st', 'right', 'bundle', 'branch', 'block', 'occasionally', 'pvcs', 'and', 'apcs', 'bp', 'ranging', '130s', '140s', '40s', '60s', 'pp', 'legs', 'and', 'arms', 'with', 'bruising', 'pt', 'with', 'pvd', 'resp', 'lungs', 'with', 'scattered', 'upper', 'airway', 'rhonchi', 'diminished', 'at', 'bases', 'pt', 'encouraged', 'to', 'cough', 'and', 'deep', 'breathe', 'using', 'incentive', 'spirometer', 'with', 'good', 'technique', 'dry', 'nonproductive', 'cough', '02', 'sat', 'down', 'to', 'mid', '80s', 'on', 'rm', 'air', 'while', 'asleep', 'uses', '4lnc', 'prn', 'while', 'sleeping', 'with', '02', 'sats', 'up', 'to', 'low', 'to', 'mid', '90s', '02', 'sat', '89', '94', 'on', 'rm', 'air', 'while', 'awake', 'o2', 'sat', 'will', 'rise', 'to', 'low', 'to', 'mid', '90s', 'with', 'deep', 'breaths', 'while', 'awake', 'with', 'coughing', 'and', 'deep', 'breathing', 'gi', 'abd', 'obese', 'hypoactive', 'bs', 'flexiseal', 'in', 'place', 'draining', 'small', 'amt', 'liquid', 'brown', 'green', 'stool', 'tolerating', 'low', 'sodium', 'cardiac', 'diet', 'in', 'afternoon', 'stool', 'more', 'formed', 'pt', 'restarted', 'on', 'bowel', 'meds', 'to', 'keep', 'stool', 'liquid', 'for', 'flexiseal', 'to', 'function', 'd', 't', 'skin', 'issues', 'gu', 'foley', 'patent', 'draining', 'cloudy', 'yellow', 'urine', '30cc', 'hr', 'skin', 'see', 'decubitus', 'ulcer', 'physical', 'therapy', 'was', 'up', 'to', 'see', 'pt', 'today', 'for', 'consult', 'physical', 'therapy', 'ordering', 'a', 'special', 'chair', 'for', 'pt', 'that', 'will', 'be', 'delivered', 'for', 'tomorrow', 'with', 'pt', 'consult', 'social', 'lived', 'with', 'son', 'in', 'trailer', 'prior', 'to', 'admission', 'pt', 'was', 'cut', 'out', 'of', 'trailer', 'd', 't', 'size', 'of', 'pt', '400', 'lbs', 'son', '30yrs', 'old', 'very', 'supportive', 'pt', 'being', 'followed', 'by', 'social', 'worker', 'and', 'psych', 'for', 'support', 'son', 'states', 'he', 'is', 'working', 'on', 'finding', 'a', 'new', 'place', 'for', 'he', 'and', 'his', 'mother', 'to', 'live', 'and', 'finding', 'a', 'job', 'nurse', 'case', 'manager', 'to', 'follow', 'pt', 'to', 'start', 'screening', 'for', 'rehab']\n"
     ]
    }
   ],
   "source": [
    "sentences = train.text.values\n",
    "tokenized_sent = []\n",
    "\n",
    "for s in sentences:\n",
    "    tokenized_sent.append(word_tokenize(s.lower()))\n",
    "print(tokenized_sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_sent)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvector_size = Dimensionality of the feature vectors.\\nwindow = The maximum distance between the current and predicted word within a sentence.\\nmin_count = Ignores all words with total frequency lower than this.\\nalpha = The initial learning rate.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d2v = Doc2Vec(tagged_data, vector_size = 200, window = 2, min_count = 10, epochs = 100)\n",
    "\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = train.copy()\n",
    "data_train['tokens'] = tokenized_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['emb'] = data_train['tokens'].apply(model_d2v.infer_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "      <th>tokens</th>\n",
       "      <th>emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>723896</td>\n",
       "      <td>full code nkda 55 y o f with morbid obesity an...</td>\n",
       "      <td>1</td>\n",
       "      <td>[full, code, nkda, 55, y, o, f, with, morbid, ...</td>\n",
       "      <td>[0.26295832, 0.011248917, 1.6407872, 0.5980760...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1853831</td>\n",
       "      <td>nursing nicu note 1 respiratory o pt remains i...</td>\n",
       "      <td>0</td>\n",
       "      <td>[nursing, nicu, note, 1, respiratory, o, pt, r...</td>\n",
       "      <td>[0.08174397, -0.87000173, 0.06789715, 0.080320...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1405842</td>\n",
       "      <td>ccu nursing progress note s i would like to ge...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ccu, nursing, progress, note, s, i, would, li...</td>\n",
       "      <td>[0.53284323, -1.7147696, 0.5414482, -0.2807709...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1573658</td>\n",
       "      <td>ccu nursing progress note s i still feel alitt...</td>\n",
       "      <td>1</td>\n",
       "      <td>[ccu, nursing, progress, note, s, i, still, fe...</td>\n",
       "      <td>[-0.2313523, -1.8485368, -1.0289205, 0.5885449...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34910</td>\n",
       "      <td>admission date 2135 12 23 discharge date 2135 ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[admission, date, 2135, 12, 23, discharge, dat...</td>\n",
       "      <td>[0.6677951, -1.1999754, -2.059161, 0.44401428,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  annotation  \\\n",
       "0   723896  full code nkda 55 y o f with morbid obesity an...           1   \n",
       "1  1853831  nursing nicu note 1 respiratory o pt remains i...           0   \n",
       "2  1405842  ccu nursing progress note s i would like to ge...           1   \n",
       "3  1573658  ccu nursing progress note s i still feel alitt...           1   \n",
       "4    34910  admission date 2135 12 23 discharge date 2135 ...           0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [full, code, nkda, 55, y, o, f, with, morbid, ...   \n",
       "1  [nursing, nicu, note, 1, respiratory, o, pt, r...   \n",
       "2  [ccu, nursing, progress, note, s, i, would, li...   \n",
       "3  [ccu, nursing, progress, note, s, i, still, fe...   \n",
       "4  [admission, date, 2135, 12, 23, discharge, dat...   \n",
       "\n",
       "                                                 emb  \n",
       "0  [0.26295832, 0.011248917, 1.6407872, 0.5980760...  \n",
       "1  [0.08174397, -0.87000173, 0.06789715, 0.080320...  \n",
       "2  [0.53284323, -1.7147696, 0.5414482, -0.2807709...  \n",
       "3  [-0.2313523, -1.8485368, -1.0289205, 0.5885449...  \n",
       "4  [0.6677951, -1.1999754, -2.059161, 0.44401428,...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 14:14:05.458267: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-21 14:14:05.612362: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-10-21 14:14:05.639363: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-21 14:14:06.135575: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-21 14:14:06.135681: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-21 14:14:06.135690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('TimKond/S-BioLinkBert-MedQuAD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.036536    0.33835596  0.51994896 ... -0.03700165  0.13714871\n",
      "  -0.07030044]\n",
      " [ 0.03993022  0.20294327  0.14775896 ... -0.1941382   0.5970359\n",
      "  -0.0982073 ]\n",
      " [-0.15195133  0.33885583  0.4040655  ... -0.18028194  0.08391595\n",
      "  -0.1527898 ]\n",
      " ...\n",
      " [ 0.16496503  0.17573677 -0.01575199 ... -0.19569075  1.0492783\n",
      "  -0.08215071]\n",
      " [ 0.02255729  0.2097973   0.08416966 ... -0.19114912  0.81562406\n",
      "  -0.07332124]\n",
      " [ 0.01963929  0.25955972  0.35870165 ... -0.06745666  0.3448465\n",
      "  -0.04506941]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "\n",
    "def clustering_question(data,emb_col,NUM_CLUSTERS = 2):\n",
    "\n",
    "    X = np.array(data[emb_col].tolist())\n",
    "\n",
    "    kclusterer = KMeansClusterer(\n",
    "        NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,\n",
    "        repeats=25,avoid_empty_clusters=True)\n",
    "\n",
    "    assigned_clusters = kclusterer.cluster(X, assign_clusters=True)\n",
    "\n",
    "    data['cluster'] = pd.Series(assigned_clusters, index=data.index)\n",
    "    data['centroid'] = data['cluster'].apply(lambda x: kclusterer.means()[x])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_d2v = clustering_question(data_train,'emb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster  annotation\n",
       "0        0             105\n",
       "         1              30\n",
       "1        0             258\n",
       "         1              32\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_d2v.groupby([\"cluster\", \"annotation\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['emb_transformer'] = data_train['text'].apply(model.encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_trans = clustering_question(data_train,'emb_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cluster  annotation\n",
       "0        0             204\n",
       "         1              45\n",
       "1        0             159\n",
       "         1              17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_trans.groupby([\"cluster\", \"annotation\"]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprerantly Very simple embeddings clustering based on doc2vec or BERT does not tell anything about the sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['positive','neutral','negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1908153</td>\n",
       "      <td>npn npn 1 remains nco2 200cc flow mostly 21 25...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1766352</td>\n",
       "      <td>dol 60 cga 36 5 28 wks cvr remains ra spells h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>696200</td>\n",
       "      <td>84 year old woman cad p cabg w stent chf dm ad...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>599709</td>\n",
       "      <td>hpi 37 yo male bicylist struck car tx scene lo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1673127</td>\n",
       "      <td>see name6 md 66 data md notes orders neuro int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>29207</td>\n",
       "      <td>admission date 2193 1 19 discharge date 2193 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>622046</td>\n",
       "      <td>renal failure acute acute renal failure arf as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2063891</td>\n",
       "      <td>npn 6 2083 2 infant conts ra rr 40 70 ls cl mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1703462</td>\n",
       "      <td>npn 0700 1900 addendum 3 fen infant tolerating...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1811694</td>\n",
       "      <td>npn 3f n infant remains gavage feeds sc26cal 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  annotation\n",
       "0    1908153  npn npn 1 remains nco2 200cc flow mostly 21 25...           1\n",
       "1    1766352  dol 60 cga 36 5 28 wks cvr remains ra spells h...           0\n",
       "2     696200  84 year old woman cad p cabg w stent chf dm ad...           0\n",
       "3     599709  hpi 37 yo male bicylist struck car tx scene lo...           0\n",
       "4    1673127  see name6 md 66 data md notes orders neuro int...           0\n",
       "..       ...                                                ...         ...\n",
       "102    29207  admission date 2193 1 19 discharge date 2193 2...           0\n",
       "103   622046  renal failure acute acute renal failure arf as...           0\n",
       "104  2063891  npn 6 2083 2 infant conts ra rr 40 70 ls cl mi...           0\n",
       "105  1703462  npn 0700 1900 addendum 3 fen infant tolerating...           0\n",
       "106  1811694  npn 3f n infant remains gavage feeds sc26cal 1...           0\n",
       "\n",
       "[107 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "netural = []\n",
    "negative = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "15\n",
      "30\n",
      "35\n",
      "38\n",
      "39\n",
      "46\n",
      "49\n",
      "52\n",
      "68\n",
      "76\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "from copy import error\n",
    "\n",
    "\n",
    "for i,row in test.iterrows():\n",
    "    try:\n",
    "        encoded_input = tokenizer(row['text'], return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        positive.append(scores[0])\n",
    "        netural.append(scores[1])\n",
    "        negative.append(scores[2])\n",
    "    except:\n",
    "        positive.append(1/3)\n",
    "        netural.append(1/3)\n",
    "        negative.append(1/3)\n",
    "        print(i)\n",
    "\n",
    "# ranking = np.argsort(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = labels[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02507626, 0.71849334, 0.25643042], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.iloc[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) neutral 0.7185\n",
      "2) negative 0.2564\n",
      "3) positive 0.0251\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Neural Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import stopwords \n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def tokenize(x_train,y_train,x_val,y_val):\n",
    "    word_list = []\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    for sent in x_train:\n",
    "        for word in sent.lower().split():\n",
    "            if word not in stop_words and word != '':\n",
    "                word_list.append(word)\n",
    "  \n",
    "    corpus = Counter(word_list)\n",
    "    # sorting on the basis of most common words\n",
    "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:5000]\n",
    "    # creating a dict\n",
    "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
    "    \n",
    "    # tockenize\n",
    "    final_list_train,final_list_test = [],[]\n",
    "    for sent in x_train:\n",
    "            final_list_train.append([onehot_dict[word] for word in sent.lower().split() \n",
    "                                     if word in onehot_dict.keys()])\n",
    "    for sent in x_val:\n",
    "            final_list_test.append([onehot_dict[word] for word in sent.lower().split() \n",
    "                                    if word in onehot_dict.keys()])\n",
    "            \n",
    "    encoded_train = [1 if label == True else 0 for label in y_train]  \n",
    "    encoded_test = [1 if label == False else 0 for label in y_val] \n",
    "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data is (425,)\n",
      "shape of test data is (107,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test = train['text'].values,test['text'].values,train['annotation'].values,test['annotation'].values\n",
    "print(f'shape of train data is {x_train.shape}')\n",
    "print(f'shape of test data is {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/gwan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24720/4037790089.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test,vocab = tokenize(x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary is 5000\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of vocabulary is {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count     425.000000\n",
       "mean      350.628235\n",
       "std       433.580995\n",
       "min        20.000000\n",
       "25%       114.000000\n",
       "50%       171.000000\n",
       "75%       321.000000\n",
       "max      2413.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_len = [len(i) for i in x_train]\n",
    "pd.Series(rev_len).hist()\n",
    "plt.show()\n",
    "pd.Series(rev_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
    "    for ii, review in enumerate(sentences):\n",
    "        if len(review) != 0:\n",
    "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = padding_(x_train,2000)\n",
    "x_test_pad = padding_(x_test,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
    "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actutal Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set([label for (label, text) in train_loader]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/gwan/Homework/hst953-2022/Modelling.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     train(train_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     accu_val \u001b[39m=\u001b[39m evaluate(valid_loader)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m total_accu \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m total_accu \u001b[39m>\u001b[39m accu_val:\n",
      "\u001b[1;32m/home/gwan/Homework/hst953-2022/Modelling.ipynb Cell 39\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m log_interval \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, (label, text, offsets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gwan/Homework/hst953-2022/Modelling.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     predicted_label \u001b[39m=\u001b[39m model(text, offsets)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_loader)\n",
    "    accu_val = evaluate(valid_loader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_tr_acc, label='Train Acc')\n",
    "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_tr_loss, label='Train loss')\n",
    "plt.plot(epoch_vl_loss, label='Validation loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(text):\n",
    "        word_seq = np.array([vocab[word] for word in text.split() \n",
    "                         if word in vocab.keys()])\n",
    "        word_seq = np.expand_dims(word_seq,axis=0)\n",
    "        pad =  torch.from_numpy(padding_(word_seq,500))\n",
    "        inputs = pad.to(device)\n",
    "        batch_size = 1\n",
    "        h = model.init_hidden(batch_size)\n",
    "        h = tuple([each.data for each in h])\n",
    "        output, h = model(inputs, h)\n",
    "        return(output.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e785c52e1a28ad980bef155e7f9c26fcc6dca216df0c4e690729e404ede1f14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
